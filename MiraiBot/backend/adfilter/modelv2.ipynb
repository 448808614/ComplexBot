{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ok\n"
    }
   ],
   "source": [
    "#%%\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "185"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "allNormalText = open(r\"./data/data-normal.txt\", 'r', encoding='utf-8').read()\n",
    "allNormalTextArray = allNormalText.split('\\n')\n",
    "len(allNormalTextArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\Kenvix\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 0.789 seconds.\nPrefix dict has been built successfully.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['没有', '上次', '学长', '学姐', '教', '几节课'],\n ['学姐', '交', '一下'],\n ['认识', '学姐', '保研'],\n ['党明', '学姐', '真的', '强'],\n ['想不到', '没有', '党明', '学姐', '做', '不到'],\n ['党明', '学姐'],\n ['党明', '学姐', '献上'],\n ['学姐'],\n ['我见', '证明', '党明', '学姐', '非常', '漂亮', '女孩子'],\n ['开机', '慢点', '还好']]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from model import AdPredictor\n",
    "adp = AdPredictor()\n",
    "splitNormalWords = [list(adp.splitWords(ad, False)) for ad in allNormalTextArray]\n",
    "splitNormalWords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)], [(2, 1), (6, 1), (7, 1)], [(2, 1), (8, 1), (9, 1)], [(2, 1), (10, 1), (11, 1), (12, 1)], [(2, 1), (5, 1), (10, 1), (13, 1), (14, 1), (15, 1)], [(2, 1), (10, 1)], [(2, 1), (10, 1), (16, 1)], [(2, 1)], [(2, 1), (10, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(22, 1), (23, 1), (24, 1)]]\n"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "# 赋给语料库中每个词(不重复的词)一个整数id\n",
    "dictionary = corpora.Dictionary(splitNormalWords)\n",
    "new_corpus = [dictionary.doc2bow(text) for text in splitNormalWords]\n",
    "print(dictionary.get(\"学姐\"))\n",
    "print(new_corpus[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 0.45687151407039367), (1, 0.45687151407039367), (2, 0.25535545406191246), (3, 0.45687151407039367), (4, 0.45687151407039367), (5, 0.3160178327324882)], [(2, 0.44996618130543437), (6, 0.556860388752772), (7, 0.6981668447581255)], [(2, 0.38899765237849326), (8, 0.6959786587088174), (9, 0.6035681677042755)], [(2, 0.3504975809135041), (10, 0.43376197428471686), (11, 0.6270959085570886), (12, 0.543831515185876)], [(2, 0.3016974387477618), (5, 0.3733688441637675), (10, 0.3733688441637675), (13, 0.42618820404090113), (14, 0.3964418831672419), (15, 0.5397846939992531)]]\n2\n153\n上次\n"
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(new_corpus)\n",
    "tfidf.save(\"tfidf.gsm\")\n",
    "\n",
    "# 使用这个训练好的模型得到单词的tfidf值\n",
    "tfidf_vec = []\n",
    "tfidf_vocabs = []\n",
    "for i in range(len(splitNormalWords)):\n",
    "    string_bow = dictionary.doc2bow(splitNormalWords[i])\n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    tfidf_vec.append(string_tfidf)\n",
    "\n",
    "print(tfidf_vec[0:5])\n",
    "tfidf_vocabs = dictionary.token2id\n",
    "print(tfidf_vocabs.get(\"学姐\"))\n",
    "print(tfidf_vocabs.get(\"大佬\"))\n",
    "# print(tfidf_vocabs)\n",
    "tfidf_vocabs = {value:key for (key, value) in tfidf_vocabs.items()}\n",
    "print(tfidf_vocabs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "185\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.02519699, -0.00593107,  0.00915863, ...,  0.01200001,\n         0.00016478, -0.0022715 ],\n       [ 0.0032771 , -0.03874716,  0.01412292, ..., -0.0296816 ,\n         0.00575662, -0.03965287],\n       [ 0.04038353, -0.04079113, -0.01122506, ..., -0.01659398,\n        -0.03109673, -0.02944126],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.03878694,  0.04019498,  0.01958669, ...,  0.04764091,\n        -0.02161361,  0.03267131],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features):\n",
    "    #print(\"tfidf_vector\", tfidf_vector)\n",
    "    word_tfidfs = []\n",
    "    word_tfidf_map = OrderedDict()\n",
    "\n",
    "    for (key, proba) in tfidf_vector:\n",
    "        word_tfidfs.append(proba)\n",
    "        word_tfidf_map[tfidf_vocabulary[key]] = proba\n",
    "    \n",
    "    # print(word_tfidfs)\n",
    "    # print(word_tfidf_map)\n",
    "    \n",
    "    feature_vector=np.zeros((num_features,),dtype='float64')\n",
    "    vocabulary=set(model.wv.index2word)\n",
    "    wts=0\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector=model[word]\n",
    "            weighted_word_vector=word_tfidf_map[word]*word_vector\n",
    "            wts=wts+word_tfidf_map[word]\n",
    "            feature_vector=np.add(feature_vector,weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector=np.divide(feature_vector,wts)\n",
    "    return feature_vector\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features):\n",
    "    docs_tfidfs=[(doc,doc_tfidf) for doc,doc_tfidf in zip(corpus,tfidf_vectors)]\n",
    "    features=[tfidf_wtd_avg_word_vectors(tokenized_sentence,tfidf,tfidf_vocabulary,model,num_features) for tokenized_sentence,tfidf in docs_tfidfs]\n",
    "    return np.array(features)\n",
    "\n",
    "# vec = Word2Vec(splitNormalWords,size=10,window=10,min_count=2,sample=1e-3)\n",
    "word2vec = Word2Vec(splitNormalWords, min_count=3, sample=1e-3, size=10, window=10)\n",
    "vected = tfidf_weighted_averaged_word_vectorizer(corpus=splitNormalWords, tfidf_vectors=tfidf_vec, tfidf_vocabulary=tfidf_vocabs,model=word2vec,num_features=10)\n",
    "# vec.build_vocab(splitNormalWords)\n",
    "# vec.train(splitNormalWords, total_examples=len(splitNormalWords), epochs=vec.epochs)\n",
    "# # vec.save(\"vec.gsm\")\n",
    "# vec\n",
    "print(len(vected))\n",
    "vected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (JupyerNotebook)",
   "language": "python",
   "name": "pycharm-87025479"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}